import Foundation
import AVFoundation
import CoreAudio
import ScreenCaptureKit
import AppKit
import CoreGraphics
import Combine
import os.log

@MainActor
final class AudioCaptureManager: NSObject, ObservableObject {
    static let shared = AudioCaptureManager()

    // MARK: - Published Properties for UI
    @Published var isListening = false
    @Published var liveTranscript = ""
    @Published var connectionStatus = "Ready"
    
    // MARK: - Debug Properties
    @Published var isAudioFlowing = false
    @Published var audioLevelDebug: Float = 0.0
    @Published var lastAudioTimestamp: Date?
    private let logger = Logger(subsystem: "com.luca.app", category: "AudioCapture")

    // MARK: - Audio Capture Properties
    private var screenCaptureSession: SCStream?
    private var audioEngine = AVAudioEngine()
    
    // ScreenCaptureKit ‚Üí Deepgram conversion
    private var scInputFormat: AVAudioFormat?
    private let dgTargetFormat: AVAudioFormat = AVAudioFormat(
        commonFormat: .pcmFormatInt16,
        sampleRate: 16_000,
        channels: 1,
        interleaved: true
    )!
    private var scToDgConverter: AVAudioConverter?
    private var micConverter: AVAudioConverter?
    
    // AVCaptureSession-based mic capture (works better with Bluetooth headsets)
    private var micCaptureSession: AVCaptureSession?
    private var micDeviceInput: AVCaptureDeviceInput?
    private var micDataOutput: AVCaptureAudioDataOutput?
    private var preferredMicDeviceUniqueID: String?
    struct MicDeviceInfo: Identifiable, Equatable {
        let id: String
        let uid: String
        let name: String
        let isDefault: Bool
    }
    @Published var availableMics: [MicDeviceInfo] = []
    
    // MARK: - STT Integration (Multi-source)
    private let multiSTT = MultiSourceSTTManager()
    private var cancellables = Set<AnyCancellable>()
    
    // Audio processing
    private var accumulatingPCM = Data()            // system buffer
    private var micAccumulatingPCM = Data()         // microphone buffer
    private var accumulatedSamples: Int = 0
    private let targetSampleRate: Double = 16_000
    private let chunkSizeBytes = DeepgramConfig.chunkSizeBytes // 20ms chunks for real-time
    private var samplesPerChunk: Int { chunkSizeBytes / 2 } // 16-bit samples
    
    // Session management
    private var sessionId: String?
    private var sessionStartTime: Date?
    private var isRunning = false
    private var lastNonSilenceAt: Date = Date()
    private var lastVoiceActivity: Date = Date()
    private var isStopping: Bool = false
    
    // VAD Configuration
    private let voiceThreshold: Float = 0.01
    
    // MARK: - Initialization
    
    override init() {
        super.init()
        setupDeepgramIntegration()
        startMonitoringDefaultInputDevice()
    }
    
    // MARK: - Public Interface
    
    /// Start listening with Deepgram STT
    func startListening(sessionId: String? = nil, onStarted: @escaping (Bool) -> Void) {
        guard !isListening else { onStarted(true); return }
        
        let sessionId = sessionId ?? UUID().uuidString
        self.sessionId = sessionId
        self.sessionStartTime = Date()
        
        Task {
            do {
                connectionStatus = "Connecting to Deepgram..."
                
                // Connect both STT sockets (system + microphone)
                multiSTT.connectAll()
                
                connectionStatus = "Starting audio capture..."
                
                // Start system audio capture
                try await startScreenCaptureWithAudio()

                // Ensure microphone permission, then start mic capture if allowed
                let micGranted = await self.ensureMicrophonePermission()
                print("üé§ DEBUG: Microphone permission granted: \(micGranted)")
                
                if micGranted {
                    do {
                        try self.startMicrophoneCapture()
                        print("‚úÖ DEBUG: Microphone capture started successfully")
                    } catch {
                        print("‚ùå DEBUG: Failed to start microphone capture: \(error.localizedDescription)")
                        print("‚ùå DEBUG: Error type: \(type(of: error))")
                        if let captureError = error as? AudioCaptureError {
                            print("‚ùå DEBUG: Capture error: \(captureError)")
                        }
                        self.logger.error("‚ùå Failed to start microphone capture: \(error.localizedDescription)")
                    }
                } else {
                    self.logger.warning("‚ö†Ô∏è Microphone permission not granted. Continuing with system audio only.")
                }
                
                isListening = true
                connectionStatus = "Listening..."
                
                // Start session in transcript store
                SessionTranscriptStore.shared.startListenSession(sessionId)
                
                onStarted(true)
                print("‚úÖ Luca: Started listening with Deepgram STT")
                
            } catch {
                connectionStatus = "Error: \(error.localizedDescription)"
                onStarted(false)
                print("‚ùå Luca: Failed to start listening: \(error)")
            }
        }
    }

    // MARK: - Permissions
    private func ensureMicrophonePermission() async -> Bool {
        let status = AVCaptureDevice.authorizationStatus(for: .audio)
        switch status {
        case .authorized:
            return true
        case .denied, .restricted:
            return false
        case .notDetermined:
            return await withCheckedContinuation { (continuation: CheckedContinuation<Bool, Never>) in
                AVCaptureDevice.requestAccess(for: .audio) { granted in
                    continuation.resume(returning: granted)
                }
            }
        @unknown default:
            return false
        }
    }
    
    /// Stop listening and save transcript
    func stopListening() async {
        guard isListening else { return }
        
        // Prevent concurrent stop calls
        if isStopping { return }
        isStopping = true
        
        // Mark as not listening immediately to gate re-entrancy
        isListening = false
        connectionStatus = "Stopping..."
        
        // Stop audio flow monitoring
        stopAudioFlowMonitoring()
        
        // Stop audio capture
        await stopScreenCapture()
        
        // Finalize Deepgram connections
        multiSTT.disconnectAll()
        
        // Stop microphone capture
        stopMicrophoneCapture()
        
        // Save transcript
        await finishSession()
        
        connectionStatus = "Ready"
        isStopping = false
        
        print("‚úÖ Luca: Stopped listening")
    }
    
    // MARK: - Deepgram Integration
    
    private func setupDeepgramIntegration() {
        // Connection status updates are managed locally for now
    }
    
    // MARK: - Session Management
    
    private func finishSession() async {
        guard let sessionId = sessionId,
              let _ = sessionStartTime else { return }
        
        // Finalize session in transcript store
        SessionTranscriptStore.shared.finalizeListenSession()
        
        // Clear session data
        self.sessionId = nil
        self.sessionStartTime = nil
        accumulatingPCM.removeAll()
        accumulatedSamples = 0
        
        print("‚úÖ Session \(sessionId) completed and saved")
    }
    
    // MARK: - Audio Capture (Enhanced for Deepgram)
    
    private func startScreenCaptureWithAudio() async throws {
        logger.info("üé¨ Starting audio capture...")
        
        // Get available content for screen + audio capture
        let availableContent = try await SCShareableContent.excludingDesktopWindows(false, onScreenWindowsOnly: true)
        
        guard let display = availableContent.displays.first else {
            logger.error("‚ùå No display available for capture")
            throw AudioCaptureError.noDisplayAvailable
        }
        
        logger.info("üñ•Ô∏è Display: \(display.width)x\(display.height)")
        
        // Check system audio devices
        let audioDevices = AVCaptureDevice.devices(for: .audio)
        logger.info("üé§ Available audio devices: \(audioDevices.count)")
        
        for device in audioDevices {
            logger.info("üì± Device: \(device.localizedName) - ID: \(device.uniqueID)")
        }
        
        // Log which applications can provide audio
        for app in availableContent.applications {
            logger.info("üì± App: \(app.applicationName) - Bundle: \(app.bundleIdentifier) - Process ID: \(app.processID)")
        }
        
        // Configure stream for screen + audio capture
        let filter = SCContentFilter(display: display, excludingWindows: [])
        let configuration = SCStreamConfiguration()
        configuration.width = Int(display.width)
        configuration.height = Int(display.height)
        
        // Audio capture settings (macOS 13.0+)
        if #available(macOS 13.0, *) {
            configuration.capturesAudio = true  // This captures system audio!
            configuration.sampleRate = Int(targetSampleRate)
            configuration.channelCount = 1
            configuration.excludesCurrentProcessAudio = true  // Don't capture Luca's own audio
        }
        
        if #available(macOS 13.0, *) {
            logger.info("‚öôÔ∏è Stream configuration - Audio: \(configuration.capturesAudio), Sample Rate: \(configuration.sampleRate), Channels: \(configuration.channelCount)")
        } else {
            logger.info("‚öôÔ∏è Stream configuration - Screen only (audio requires macOS 13.0+)")
        }
        
        // Create and start the capture stream
        screenCaptureSession = SCStream(filter: filter, configuration: configuration, delegate: self)
        
        // Add stream outputs
        if #available(macOS 13.0, *) {
            try screenCaptureSession?.addStreamOutput(self, type: .audio, sampleHandlerQueue: .main)
        }
        // Attach a screen output to suppress ScreenCaptureKit warnings even if we ignore frames
        try? screenCaptureSession?.addStreamOutput(self, type: .screen, sampleHandlerQueue: .main)
        
        // Start capture
        try await screenCaptureSession?.startCapture()
        
        // Start audio flow monitoring
        checkAudioFlow()
        
        logger.info("‚úÖ System audio capture started for Deepgram STT")
        print("‚úÖ System audio capture started for Deepgram STT")
    }
    
    // MARK: - Audio Flow Monitoring
    
    private var audioFlowTimer: Timer?
    
    private func checkAudioFlow() {
        // Stop any existing timer first
        audioFlowTimer?.invalidate()
        
        audioFlowTimer = Timer.scheduledTimer(withTimeInterval: 5.0, repeats: true) { [weak self] _ in
            guard let self = self else { return }
            
            // Simple check without expensive Task creation
            DispatchQueue.main.async {
                if let lastTimestamp = self.lastAudioTimestamp {
                    let timeSinceLastAudio = Date().timeIntervalSince(lastTimestamp)
                    if timeSinceLastAudio > 5.0 {
                        self.logger.warning("‚ö†Ô∏è No audio received for \(String(format: "%.1f", timeSinceLastAudio)) seconds")
                        self.isAudioFlowing = false
                    }
                } else {
                    self.logger.warning("‚ö†Ô∏è No audio has been received yet")
                    self.isAudioFlowing = false
                }
            }
        }
    }
    
    private func stopAudioFlowMonitoring() {
        audioFlowTimer?.invalidate()
        audioFlowTimer = nil
    }
    
    private func stopScreenCapture() async {
        guard let session = screenCaptureSession else { return }
        
        do {
            try await session.stopCapture()
            screenCaptureSession = nil
            print("‚úÖ System audio capture stopped")
        } catch {
            print("‚ö†Ô∏è Error stopping screen capture: \(error)")
        }
    }
    
    // MARK: - Microphone Capture (AVAudioEngine)
    private func startMicrophoneCapture() throws {
        print("üé§ DEBUG: Starting microphone capture...")
        // Prefer AVCaptureSession to better follow Bluetooth headset mics
        stopMicrophoneCapture()
        
        let session = AVCaptureSession()
        session.beginConfiguration()
        
        // Select best input device (prefer Bluetooth headsets when present)
        guard let device = selectInputDevice() else {
            print("‚ùå DEBUG: No audio capture device available")
            // List all available devices for debugging
            let allDevices = AVCaptureDevice.devices(for: .audio)
            print("üé§ DEBUG: Available audio devices: \(allDevices.count)")
            for dev in allDevices {
                print("üé§ DEBUG: - \(dev.localizedName) [\(dev.uniqueID)]")
            }
            logger.error("‚ùå No audio capture device available")
            throw AudioCaptureError.audioEngineFailed
        }
        
        print("üé§ DEBUG: Selected device: \(device.localizedName)")
        print("üé§ DEBUG: Device formats: \(device.formats.count)")
        
        let input = try AVCaptureDeviceInput(device: device)
        print("üé§ DEBUG: Created device input successfully")
        
        guard session.canAddInput(input) else {
            print("‚ùå DEBUG: Cannot add microphone input to capture session")
            print("‚ùå DEBUG: Session preset: \(session.sessionPreset)")
            logger.error("‚ùå Cannot add microphone input to capture session")
            throw AudioCaptureError.audioEngineFailed
        }
        session.addInput(input)
        print("‚úÖ DEBUG: Added input to session")
        
        let output = AVCaptureAudioDataOutput()
        let queue = DispatchQueue(label: "mic.capture.queue", qos: .userInitiated)
        output.setSampleBufferDelegate(self, queue: queue)
        print("üé§ DEBUG: Created audio output with delegate")
        
        guard session.canAddOutput(output) else {
            print("‚ùå DEBUG: Cannot add microphone output to capture session")
            logger.error("‚ùå Cannot add microphone output to capture session")
            throw AudioCaptureError.audioEngineFailed
        }
        session.addOutput(output)
        print("‚úÖ DEBUG: Added output to session")
        
        session.commitConfiguration()
        print("üé§ DEBUG: Session configuration committed")
        
        session.startRunning()
        print("üé§ DEBUG: Session started running: \(session.isRunning)")
        
        micCaptureSession = session
        micDeviceInput = input
        micDataOutput = output
        availableMics = enumerateInputDevices()
        
        // Debug the actual audio format being used
        if let connection = output.connections.first {
            print("üé§ DEBUG: Connection established: \(connection)")
            print("üé§ DEBUG: Connection enabled: \(connection.isEnabled)")
            print("üé§ DEBUG: Connection active: \(connection.isActive)")
        }
        
        logger.info("üé§ Microphone capture started (AVCaptureSession) - device: \(device.localizedName)")
        logger.info("üé§ Microphone session running: \(session.isRunning)")
    }

    // Prefer current system default input; fall back to discovery/heuristics
    private func selectInputDevice() -> AVCaptureDevice? {
        // Log current system default and all available audio devices for debugging
        let currentDefault = AVCaptureDevice.default(for: .audio)
        logger.info("üéß Current system default: \(currentDefault?.localizedName ?? "None")")
        let all = AVCaptureDevice.devices(for: .audio)
        for dev in all {
            logger.info("üé§ Detected input device: \(dev.localizedName) [\(dev.uniqueID)]")
        }
        availableMics = enumerateInputDevices()
        // 0) If we previously selected a headset and it's still present, stick to it
        if let preferredID = preferredMicDeviceUniqueID, let stillPresent = all.first(where: { $0.uniqueID == preferredID }) {
            logger.info("üéß Using previously selected headset: \(stillPresent.localizedName)")
            return stillPresent
        }
        // 1) Prefer known headset names even if not system default
        let preferredKeywords = ["AirPods", "Bluetooth", "Headset", "WH-", "WF-", "Sony", "Bose", "JBL", "Beats", "Earbuds"]
        if let named = all.first(where: { dev in
            preferredKeywords.contains(where: { dev.localizedName.localizedCaseInsensitiveContains($0) })
        }) {
            preferredMicDeviceUniqueID = named.uniqueID
            logger.info("üéß Selecting headset over default: \(named.localizedName)")
            return named
        }
        // 2) Prefer the current system default input
        if let defaultDevice = AVCaptureDevice.default(for: .audio) {
            logger.info("üéß Using system default input: \(defaultDevice.localizedName)")
            return defaultDevice
        }
        // 2) Fallback to discovery + name heuristics for common headsets
        let discovery = AVCaptureDevice.DiscoverySession(
            deviceTypes: [.builtInMicrophone, .externalUnknown],
            mediaType: .audio,
            position: .unspecified
        )
        // As a last resort, pick the first discovered device
        if let any = discovery.devices.first {
            logger.info("üéß Using discovered device: \(any.localizedName)")
            return any
        }
        // 3) Final fallback: ask for default again
        return AVCaptureDevice.default(for: .audio)
    }
    
    private func stopMicrophoneCapture() {
        if let session = micCaptureSession, session.isRunning {
            session.stopRunning()
        }
        micCaptureSession = nil
        micDeviceInput = nil
        micDataOutput = nil
        logger.info("üõë Microphone capture stopped")
    }

    // MARK: - Device Enumeration (CoreAudio - shows Bluetooth, iPhone, virtuals)
    private func enumerateInputDevices() -> [MicDeviceInfo] {
        var dataSize: UInt32 = 0
        var address = AudioObjectPropertyAddress(
            mSelector: kAudioHardwarePropertyDevices,
            mScope: kAudioObjectPropertyScopeGlobal,
            mElement: kAudioObjectPropertyElementMain
        )
        var status = AudioObjectGetPropertyDataSize(AudioObjectID(kAudioObjectSystemObject), &address, 0, nil, &dataSize)
        guard status == noErr else { return [] }
        let deviceCount = Int(dataSize) / MemoryLayout<AudioDeviceID>.size
        var deviceIDs = Array(repeating: AudioDeviceID(), count: deviceCount)
        status = AudioObjectGetPropertyData(AudioObjectID(kAudioObjectSystemObject), &address, 0, nil, &dataSize, &deviceIDs)
        guard status == noErr else { return [] }
        
        // Get default input device id
        var defaultInputId = AudioDeviceID(0)
        var defSize = UInt32(MemoryLayout<AudioDeviceID>.size)
        var defAddr = AudioObjectPropertyAddress(
            mSelector: kAudioHardwarePropertyDefaultInputDevice,
            mScope: kAudioObjectPropertyScopeGlobal,
            mElement: kAudioObjectPropertyElementMain
        )
        _ = AudioObjectGetPropertyData(AudioObjectID(kAudioObjectSystemObject), &defAddr, 0, nil, &defSize, &defaultInputId)
        
        var results: [MicDeviceInfo] = []
        for devId in deviceIDs {
            // Check if device has input scope streams
            var streamAddr = AudioObjectPropertyAddress(
                mSelector: kAudioDevicePropertyStreams,
                mScope: kAudioDevicePropertyScopeInput,
                mElement: kAudioObjectPropertyElementMain
            )
            var streamsSize: UInt32 = 0
            if AudioObjectGetPropertyDataSize(devId, &streamAddr, 0, nil, &streamsSize) != noErr || streamsSize == 0 {
                continue
            }
            // Name
            var name: CFString = "" as CFString
            var nameSize = UInt32(MemoryLayout<CFString>.size)
            var nameAddr = AudioObjectPropertyAddress(
                mSelector: kAudioObjectPropertyName,
                mScope: kAudioObjectPropertyScopeGlobal,
                mElement: kAudioObjectPropertyElementMain
            )
            _ = AudioObjectGetPropertyData(devId, &nameAddr, 0, nil, &nameSize, &name)
            
            // UID
            var uid: CFString = "" as CFString
            var uidSize = UInt32(MemoryLayout<CFString>.size)
            var uidAddr = AudioObjectPropertyAddress(
                mSelector: kAudioDevicePropertyDeviceUID,
                mScope: kAudioObjectPropertyScopeGlobal,
                mElement: kAudioObjectPropertyElementMain
            )
            _ = AudioObjectGetPropertyData(devId, &uidAddr, 0, nil, &uidSize, &uid)
            
            let info = MicDeviceInfo(
                id: (uid as String),
                uid: (uid as String),
                name: (name as String),
                isDefault: devId == defaultInputId
            )
            results.append(info)
        }
        // Sort default first, then by name
        results.sort { lhs, rhs in
            if lhs.isDefault != rhs.isDefault { return lhs.isDefault && !rhs.isDefault }
            return lhs.name.localizedCaseInsensitiveCompare(rhs.name) == .orderedAscending
        }
        return results
    }
    
    // Debug counters for microphone processing
    private var micBufferCount = 0
    private var micProcessCount = 0
    private var micSampleCount = 0
    
    private func processMicBuffer(_ srcBuffer: AVAudioPCMBuffer, sourceFormat: AVAudioFormat) {
        micBufferCount += 1
        
        guard isListening else { 
            print("‚ö†Ô∏è DEBUG: Not listening, skipping mic buffer processing")
            return 
        }
        
        if micBufferCount % 20 == 0 {
            print("üé§ DEBUG: Processing mic audio buffer #\(micBufferCount)")
            print("üé§ DEBUG: Source format - SR: \(sourceFormat.sampleRate), Ch: \(sourceFormat.channelCount)")
        }
        
        if micConverter == nil || micConverter?.inputFormat != sourceFormat {
            micConverter = AVAudioConverter(from: sourceFormat, to: dgTargetFormat)
            print("üîÅ DEBUG: Created Mic‚ÜíDG converter: \(sourceFormat.sampleRate)Hz \(sourceFormat.channelCount)ch ‚Üí 16kHz 1ch")
            
            // Verify converter was created successfully
            if micConverter == nil {
                print("‚ùå DEBUG: Failed to create mic audio converter!")
                return
            }
            logger.info("üîÅ Created Mic‚ÜíDG converter: src sr=\(sourceFormat.sampleRate), ch=\(sourceFormat.channelCount)")
        }
        guard let converter = micConverter else { 
            print("‚ùå DEBUG: Mic converter unavailable")
            return 
        }
        
        let frameCapacity = AVAudioFrameCount(Double(srcBuffer.frameLength) * dgTargetFormat.sampleRate / sourceFormat.sampleRate) + 1
        guard let outBuffer = AVAudioPCMBuffer(pcmFormat: dgTargetFormat, frameCapacity: frameCapacity) else { 
            print("‚ùå DEBUG: Failed to create mic output buffer")
            return 
        }
        
        var convError: NSError?
        let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
            outStatus.pointee = .haveData
            return srcBuffer
        }
        converter.convert(to: outBuffer, error: &convError, withInputFrom: inputBlock)
        if let e = convError {
            print("‚ùå DEBUG: Mic convert error: \(e.localizedDescription)")
            logger.error("‚ùå Mic convert error: \(e.localizedDescription)")
            return
        }
        
        guard let ch = outBuffer.int16ChannelData else { 
            print("‚ùå DEBUG: No int16 channel data after mic conversion")
            return 
        }
        
        let sampleCount = Int(outBuffer.frameLength)
        let samples = Array(UnsafeBufferPointer(start: ch[0], count: sampleCount))
        let sampleData = Data(bytes: samples, count: samples.count * MemoryLayout<Int16>.size)
        
        if micBufferCount % 20 == 0 {
            print("üé§ DEBUG: Mic converted \(sampleCount) samples to \(sampleData.count) bytes")
        }
        
        micAccumulatingPCM.append(sampleData)
        while micAccumulatingPCM.count >= chunkSizeBytes {
            let chunk = micAccumulatingPCM.prefix(chunkSizeBytes)
            micAccumulatingPCM.removeFirst(chunkSizeBytes)
            
            print("üì° DEBUG: Sending microphone chunk: \(chunk.count) bytes to Deepgram")
            logger.debug("üé§ Sending microphone audio chunk: \(chunk.count) bytes")
            multiSTT.sendAudioData(Data(chunk), for: .microphone)
        }
    }

    private func processMicCMSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
        micProcessCount += 1
        
        if micProcessCount % 25 == 0 {
            print("üé§ DEBUG: Processing mic buffer #\(micProcessCount)")
        }
        
        guard isListening else { 
            print("‚ö†Ô∏è DEBUG: Not listening, skipping mic buffer processing")
            return 
        }
        
        guard let formatDesc = CMSampleBufferGetFormatDescription(sampleBuffer),
              let asbdPtr = CMAudioFormatDescriptionGetStreamBasicDescription(formatDesc) else { 
            print("‚ùå DEBUG: No format description in mic sample buffer")
            return 
        }
        
        var asbd = asbdPtr.pointee
        guard let sourceFormat = AVAudioFormat(streamDescription: &asbd) else { 
            print("‚ùå DEBUG: Failed to create AVAudioFormat from mic sample")
            return 
        }
        
        if micProcessCount % 25 == 0 {
            print("üé§ DEBUG: Mic source format - SR: \(sourceFormat.sampleRate), Ch: \(sourceFormat.channelCount)")
        }
        
        let frameCount = AVAudioFrameCount(CMSampleBufferGetNumSamples(sampleBuffer))
        guard let srcBuffer = AVAudioPCMBuffer(pcmFormat: sourceFormat, frameCapacity: frameCount) else { 
            print("‚ùå DEBUG: Failed to allocate mic PCM buffer")
            return 
        }
        
        srcBuffer.frameLength = frameCount
        let cmStatus = CMSampleBufferCopyPCMDataIntoAudioBufferList(sampleBuffer, at: 0, frameCount: Int32(frameCount), into: srcBuffer.mutableAudioBufferList)
        if cmStatus != noErr { 
            print("‚ùå DEBUG: Failed to copy mic PCM data (status: \(cmStatus))")
            return 
        }
        
        processMicBuffer(srcBuffer, sourceFormat: sourceFormat)
    }

    // MARK: - Audio Processing (from screen capture)
    
    private func processAudioFromSampleBuffer(_ sampleBuffer: CMSampleBuffer) {
        guard isListening else { return }
        
        // Update audio flow status without expensive logging
        lastAudioTimestamp = Date()
        isAudioFlowing = true
        
        // Discover source format from CMSampleBuffer
        guard let formatDesc = CMSampleBufferGetFormatDescription(sampleBuffer),
              let asbdPtr = CMAudioFormatDescriptionGetStreamBasicDescription(formatDesc) else {
            logger.error("‚ùå No audio format description")
            return
        }
        var asbd = asbdPtr.pointee
        guard let sourceFormat = AVAudioFormat(streamDescription: &asbd) else {
            logger.error("‚ùå Failed to create source AVAudioFormat")
            return
        }
        scInputFormat = sourceFormat
        
        // Build an AVAudioPCMBuffer and copy PCM from CMSampleBuffer safely
        let frameCount = AVAudioFrameCount(CMSampleBufferGetNumSamples(sampleBuffer))
        guard let srcBuffer = AVAudioPCMBuffer(pcmFormat: sourceFormat, frameCapacity: frameCount) else {
            logger.error("‚ùå Failed to allocate source AVAudioPCMBuffer")
            return
        }
        srcBuffer.frameLength = frameCount
        // Use CoreMedia to copy PCM correctly (handles interleaved/planar and channel counts)
        let cmStatus = CMSampleBufferCopyPCMDataIntoAudioBufferList(
            sampleBuffer,
            at: 0,
            frameCount: Int32(frameCount),
            into: srcBuffer.mutableAudioBufferList
        )
        if cmStatus != noErr {
            logger.error("‚ùå Failed to copy PCM into AVAudioPCMBuffer (status: \(cmStatus))")
            return
        }
        
        // Create or reuse converter to 16k mono Int16
        if scToDgConverter == nil || scToDgConverter?.inputFormat != sourceFormat {
            scToDgConverter = AVAudioConverter(from: sourceFormat, to: dgTargetFormat)
            logger.info("üîÅ Created SC‚ÜíDG converter: src sr=\(sourceFormat.sampleRate), ch=\(sourceFormat.channelCount), fmt=\(sourceFormat.commonFormat.rawValue) ‚Üí 16k mono int16")
        }
        guard let converter = scToDgConverter,
              let outBuffer = AVAudioPCMBuffer(pcmFormat: dgTargetFormat, frameCapacity: frameCount) else {
            logger.error("‚ùå Converter or output buffer unavailable")
            return
        }
        
        var convError: NSError?
        let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
            outStatus.pointee = .haveData
            return srcBuffer
        }
        converter.convert(to: outBuffer, error: &convError, withInputFrom: inputBlock)
        if let e = convError {
            logger.error("‚ùå Audio convert error: \(e.localizedDescription)")
            return
        }
        
        // Extract converted Int16 samples
        guard let ch = outBuffer.int16ChannelData else {
            logger.error("‚ùå No int16 channel data after conversion")
            return
        }
        let sampleCount = Int(outBuffer.frameLength)
        let samples = Array(UnsafeBufferPointer(start: ch[0], count: sampleCount))
        let sampleData = Data(bytes: samples, count: samples.count * MemoryLayout<Int16>.size)
        accumulatingPCM.append(sampleData)
        accumulatedSamples += samples.count
        
        // Audio level for debug (converted domain) - only update occasionally to reduce UI load
        let audioLevel = calculateAudioLevel(samples: samples)
        if accumulatedSamples % 100 == 0 { // Only update every 100 samples
            DispatchQueue.main.async { self.audioLevelDebug = audioLevel }
        }
        // Remove expensive debug logging
        // logger.debug("üîä Audio level: \(audioLevel) dB, Samples: \(samples.count), Accum: \(self.accumulatingPCM.count) bytes")
        
        // Ensure 16-bit alignment
        assert(accumulatingPCM.count % 2 == 0, "PCM buffer misaligned (expected even number of bytes)")
        
        // Chunk and send (20ms = 640 bytes @ 16k mono int16) - real-time
        while accumulatingPCM.count >= chunkSizeBytes {
            let chunk = accumulatingPCM.prefix(chunkSizeBytes)
            accumulatingPCM.removeFirst(chunkSizeBytes)
            let dataChunk = Data(chunk)
            multiSTT.sendAudioData(dataChunk, for: .system)
            // Removed expensive debug logging for performance
        }
        
        // Voice activity detection (converted samples)
        let rms = calculateRMS(samples)
        if rms > voiceThreshold {
            lastVoiceActivity = Date()
            lastNonSilenceAt = Date()
            logger.debug("üé§ Voice activity detected - RMS: \(rms)")
        }
    }
    
    // MARK: - Audio Level Calculation
    
    private func calculateAudioLevel(samples: [Int16]) -> Float {
        guard !samples.isEmpty else { return -100.0 }
        
        let sum = samples.reduce(0.0) { result, sample in
            return result + (Float(sample) * Float(sample))
        }
        
        let rms = sqrt(sum / Float(samples.count))
        let db = 20.0 * log10(rms / 32767.0)
        
        return db
    }
    
    private func calculateRMS(_ samples: [Int16]) -> Float {
        guard !samples.isEmpty else { return 0.0 }
        
        // Convert to Double first to avoid Int16 overflow, then square
        // Use a more robust approach to prevent any potential overflow
        var sum: Double = 0.0
        for sample in samples {
            let sampleDouble = Double(sample)
            sum += sampleDouble * sampleDouble
        }
        
        let rms = sqrt(sum / Double(samples.count))
        return Float(rms)
    }
    
    // MARK: - Force Cleanup (for app termination)
    
    func forceCleanup() {
        print("üîí Force cleanup requested - stopping all audio capture")
        Task {
            await stopListening()
        }
        
        // Additional force cleanup for app termination
        Task {
            // Force stop screen capture if still running
            if let session = screenCaptureSession {
                try? await session.stopCapture()
                screenCaptureSession = nil
            }
            
            // Force stop audio engine
            if audioEngine.isRunning {
                audioEngine.stop()
            }
            
            // Force disconnect Deepgram (both sources)
            multiSTT.disconnectAll()
            
            print("‚úÖ Force cleanup completed")
        }
    }

    // MARK: - Default Input Device Monitoring (auto-switch to headphones mic)
    private var inputDeviceListenerAdded = false
    private func startMonitoringDefaultInputDevice() {
        guard !inputDeviceListenerAdded else { return }
        let systemObjectID = AudioObjectID(kAudioObjectSystemObject)
        var address = AudioObjectPropertyAddress(
            mSelector: kAudioHardwarePropertyDefaultInputDevice,
            mScope: kAudioObjectPropertyScopeGlobal,
            mElement: kAudioObjectPropertyElementMain
        )
        let queue = DispatchQueue.main
        let status = AudioObjectAddPropertyListenerBlock(systemObjectID, &address, queue) { [weak self] _, _ in
            guard let self = self else { return }
            self.logger.info("üîÑ Default input device changed. Restarting mic capture...")
            // Only restart if we are actively listening
            if self.isListening {
                self.stopMicrophoneCapture()
                do {
                    try self.startMicrophoneCapture()
                    if let name = self.currentDefaultInputDeviceName() {
                        self.logger.info("üéß Switched to input device: \(name)")
                    }
                } catch {
                    self.logger.error("‚ùå Failed to restart mic after device change: \(error.localizedDescription)")
                }
            } else {
                self.logger.info("‚ÑπÔ∏è Not listening; skipping mic restart on device change")
            }
        }
        if status == noErr {
            inputDeviceListenerAdded = true
            if let name = currentDefaultInputDeviceName() {
                logger.info("üéß Initial input device: \(name)")
            }
        } else {
            logger.error("‚ùå Failed adding default input device listener (status=\(status))")
        }
    }
    
    private func currentDefaultInputDeviceName() -> String? {
        var deviceID = AudioDeviceID(0)
        var address = AudioObjectPropertyAddress(
            mSelector: kAudioHardwarePropertyDefaultInputDevice,
            mScope: kAudioObjectPropertyScopeGlobal,
            mElement: kAudioObjectPropertyElementMain
        )
        var dataSize = UInt32(MemoryLayout<AudioDeviceID>.size)
        let status = AudioObjectGetPropertyData(AudioObjectID(kAudioObjectSystemObject), &address, 0, nil, &dataSize, &deviceID)
        guard status == noErr else { return nil }
        
        var nameAddress = AudioObjectPropertyAddress(
            mSelector: kAudioObjectPropertyName,
            mScope: kAudioObjectPropertyScopeGlobal,
            mElement: kAudioObjectPropertyElementMain
        )
        var cfName: CFString = "" as CFString
        var nameSize = UInt32(MemoryLayout<CFString>.size)
        let nameStatus = AudioObjectGetPropertyData(deviceID, &nameAddress, 0, nil, &nameSize, &cfName)
        if nameStatus == noErr {
            return cfName as String
        }
        return nil
    }
}

// MARK: - SCStreamDelegate

@MainActor
extension AudioCaptureManager: @preconcurrency SCStreamDelegate, @preconcurrency SCStreamOutput, AVCaptureAudioDataOutputSampleBufferDelegate {
    func stream(_ stream: SCStream, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, of type: SCStreamOutputType) {
        switch type {
        case .audio:
            processAudioFromSampleBuffer(sampleBuffer)
        case .screen:
            // Handle screen updates if needed
            break
        case .microphone:
            // Handle microphone input if needed
            break
        @unknown default:
            print("‚ö†Ô∏è Unknown sample buffer type: \(type)")
        }
    }
    
    // AVCaptureAudioDataOutputSampleBufferDelegate
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        // Add counter to avoid spam
        micSampleCount += 1
        
        if micSampleCount % 50 == 0 { // Log every 50th sample
            print("üé§ DEBUG: Microphone sample #\(micSampleCount) received")
            
            // Debug sample buffer details
            let numSamples = CMSampleBufferGetNumSamples(sampleBuffer)
            print("üé§ DEBUG: Sample buffer samples: \(numSamples)")
            
            if let formatDesc = CMSampleBufferGetFormatDescription(sampleBuffer) {
                print("üé§ DEBUG: Has format description")
                if let asbd = CMAudioFormatDescriptionGetStreamBasicDescription(formatDesc) {
                    print("üé§ DEBUG: Sample rate: \(asbd.pointee.mSampleRate)")
                    print("üé§ DEBUG: Channels: \(asbd.pointee.mChannelsPerFrame)")
                    print("üé§ DEBUG: Format ID: \(asbd.pointee.mFormatID)")
                }
            }
        }
        
        guard isListening else { 
            if micSampleCount % 100 == 0 {
                print("‚ö†Ô∏è DEBUG: Received mic sample but not listening")
            }
            return 
        }
        
        logger.debug("üé§ Microphone audio sample received")
        processMicCMSampleBuffer(sampleBuffer)
    }
    
    func stream(_ stream: SCStream, didStopWithError error: Error) {
        print("‚ùå Screen capture stream stopped with error: \(error)")
        isRunning = false
    }
}

// MARK: - Audio Capture Errors

enum AudioCaptureError: Error, LocalizedError {
    case noDisplayAvailable
    case permissionDenied
    case audioEngineFailed
    
    var errorDescription: String? {
        switch self {
        case .noDisplayAvailable:
            return "No display available for capture"
        case .permissionDenied:
            return "Screen recording permission denied"
        case .audioEngineFailed:
            return "Audio engine failed to start"
        }
    }
}